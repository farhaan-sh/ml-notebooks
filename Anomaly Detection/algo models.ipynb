{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Isolation Forests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isolation Forest Predicted Outliers: (array([  5,  38,  58,  92,  94, 109, 115, 126, 148, 186, 218, 220, 239,\n",
      "       252, 254, 290, 319, 333, 377, 389, 404, 416, 437, 473, 477, 490,\n",
      "       520, 529, 557, 563, 567, 578, 583, 595, 614, 652, 682, 694, 707,\n",
      "       767, 791, 838, 852, 855, 856, 889, 908, 914, 941, 987], dtype=int64),)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "\n",
    "# Create a synthetic dataset with anomalies\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=8, n_redundant=2, random_state=42)\n",
    "# Introduce anomalies by replacing values in some instances with extreme values\n",
    "X[5, :] = np.random.normal(loc=100, scale=10, size=10)\n",
    "\n",
    "# Fit Isolation Forest model\n",
    "clf_if = IsolationForest(contamination=0.05, random_state=42)\n",
    "clf_if.fit(X)\n",
    "\n",
    "# Predict outliers\n",
    "y_pred_if = clf_if.predict(X)\n",
    "\n",
    "# Print predicted outliers\n",
    "print(\"Isolation Forest Predicted Outliers:\", np.where(y_pred_if == -1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **One-Class SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Class SVM Predicted Outliers: (array([  5,  38,  58,  92,  94, 115, 126, 159, 186, 220, 252, 254, 263,\n",
      "       290, 307, 319, 377, 389, 404, 439, 459, 473, 529, 557, 563, 567,\n",
      "       578, 579, 583, 595, 618, 652, 666, 672, 682, 692, 694, 707, 767,\n",
      "       791, 797, 816, 852, 855, 856, 908, 914, 967, 973, 987], dtype=int64),)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "# Fit One-Class SVM model\n",
    "clf_ocsvm = OneClassSVM(nu=0.05)\n",
    "clf_ocsvm.fit(X)\n",
    "\n",
    "# Predict outliers\n",
    "y_pred_ocsvm = clf_ocsvm.predict(X)\n",
    "\n",
    "# Print predicted outliers\n",
    "print(\"One-Class SVM Predicted Outliers:\", np.where(y_pred_ocsvm == -1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Autoencoders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:From c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "25/25 [==============================] - 3s 35ms/step - loss: 1.3935 - val_loss: 0.5605\n",
      "Epoch 2/50\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 1.3652 - val_loss: 0.5412\n",
      "Epoch 3/50\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 1.3441 - val_loss: 0.5219\n",
      "Epoch 4/50\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 1.3228 - val_loss: 0.5020\n",
      "Epoch 5/50\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 1.3012 - val_loss: 0.4817\n",
      "Epoch 6/50\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 1.2811 - val_loss: 0.4621\n",
      "Epoch 7/50\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 1.2616 - val_loss: 0.4429\n",
      "Epoch 8/50\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 1.2428 - val_loss: 0.4245\n",
      "Epoch 9/50\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 1.2253 - val_loss: 0.4074\n",
      "Epoch 10/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 1.2091 - val_loss: 0.3918\n",
      "Epoch 11/50\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 1.1943 - val_loss: 0.3776\n",
      "Epoch 12/50\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 1.1811 - val_loss: 0.3649\n",
      "Epoch 13/50\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 1.1693 - val_loss: 0.3537\n",
      "Epoch 14/50\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 1.1590 - val_loss: 0.3437\n",
      "Epoch 15/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 1.1499 - val_loss: 0.3350\n",
      "Epoch 16/50\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 1.1420 - val_loss: 0.3274\n",
      "Epoch 17/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 1.1351 - val_loss: 0.3207\n",
      "Epoch 18/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 1.1291 - val_loss: 0.3148\n",
      "Epoch 19/50\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 1.1238 - val_loss: 0.3095\n",
      "Epoch 20/50\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 1.1191 - val_loss: 0.3050\n",
      "Epoch 21/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 1.1149 - val_loss: 0.3009\n",
      "Epoch 22/50\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 1.1111 - val_loss: 0.2972\n",
      "Epoch 23/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 1.1078 - val_loss: 0.2939\n",
      "Epoch 24/50\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 1.1047 - val_loss: 0.2908\n",
      "Epoch 25/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 1.1019 - val_loss: 0.2880\n",
      "Epoch 26/50\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 1.0992 - val_loss: 0.2854\n",
      "Epoch 27/50\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 1.0968 - val_loss: 0.2829\n",
      "Epoch 28/50\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 1.0945 - val_loss: 0.2806\n",
      "Epoch 29/50\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 1.0923 - val_loss: 0.2784\n",
      "Epoch 30/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 1.0902 - val_loss: 0.2763\n",
      "Epoch 31/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 1.0882 - val_loss: 0.2744\n",
      "Epoch 32/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 1.0863 - val_loss: 0.2725\n",
      "Epoch 33/50\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 1.0845 - val_loss: 0.2706\n",
      "Epoch 34/50\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 1.0827 - val_loss: 0.2689\n",
      "Epoch 35/50\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 1.0810 - val_loss: 0.2672\n",
      "Epoch 36/50\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 1.0793 - val_loss: 0.2656\n",
      "Epoch 37/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 1.0777 - val_loss: 0.2640\n",
      "Epoch 38/50\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 1.0761 - val_loss: 0.2625\n",
      "Epoch 39/50\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 1.0746 - val_loss: 0.2609\n",
      "Epoch 40/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 1.0730 - val_loss: 0.2595\n",
      "Epoch 41/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 1.0715 - val_loss: 0.2580\n",
      "Epoch 42/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 1.0701 - val_loss: 0.2566\n",
      "Epoch 43/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 1.0686 - val_loss: 0.2552\n",
      "Epoch 44/50\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 1.0672 - val_loss: 0.2538\n",
      "Epoch 45/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 1.0658 - val_loss: 0.2525\n",
      "Epoch 46/50\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 1.0645 - val_loss: 0.2512\n",
      "Epoch 47/50\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 1.0632 - val_loss: 0.2499\n",
      "Epoch 48/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 1.0619 - val_loss: 0.2486\n",
      "Epoch 49/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 1.0606 - val_loss: 0.2474\n",
      "Epoch 50/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 1.0593 - val_loss: 0.2461\n",
      "32/32 [==============================] - 0s 3ms/step\n",
      "Autoencoder Predicted Outliers: (array([  5,  50,  58,  62,  94, 115, 126, 148, 186, 218, 254, 263, 307,\n",
      "       319, 377, 389, 404, 416, 437, 439, 473, 477, 520, 529, 557, 563,\n",
      "       567, 578, 583, 595, 666, 682, 692, 694, 767, 791, 797, 816, 827,\n",
      "       838, 852, 856, 889, 890, 908, 946, 962, 967, 973, 987], dtype=int64),)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Define Autoencoder model\n",
    "input_layer = Input(shape=(10,))\n",
    "encoded = Dense(8, activation='relu')(input_layer)\n",
    "decoded = Dense(10, activation='sigmoid')(encoded)\n",
    "\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Fit Autoencoder model\n",
    "autoencoder.fit(X_scaled, X_scaled, epochs=50, batch_size=32, shuffle=True, validation_split=0.2)\n",
    "\n",
    "# Predict reconstruction errors\n",
    "X_pred = autoencoder.predict(X_scaled)\n",
    "mse = np.mean(np.square(X_scaled - X_pred), axis=1)\n",
    "\n",
    "# Set a threshold for anomaly detection\n",
    "threshold = np.percentile(mse, 95)\n",
    "\n",
    "# Predict outliers\n",
    "y_pred_autoencoder = (mse > threshold).astype(int)\n",
    "\n",
    "# Print predicted outliers\n",
    "print(\"Autoencoder Predicted Outliers:\", np.where(y_pred_autoencoder == 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Explanation**\n",
    "\n",
    "\n",
    "```python\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "```\n",
    "\n",
    "- Import necessary modules:\n",
    "  - `Input` and `Dense` are layers from Keras for building neural networks.\n",
    "  - `Model` is used to create a Keras model.\n",
    "  - `StandardScaler` from scikit-learn is imported for data standardization.\n",
    "\n",
    "```python\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "```\n",
    "\n",
    "- Standardize the input data using `StandardScaler`:\n",
    "  - Create a `StandardScaler` object.\n",
    "  - Fit the scaler to the data (`X`) and transform the data to have zero mean and unit variance.\n",
    "\n",
    "```python\n",
    "# Define Autoencoder model\n",
    "input_layer = Input(shape=(10,))\n",
    "encoded = Dense(8, activation='relu')(input_layer)\n",
    "decoded = Dense(10, activation='sigmoid')(encoded)\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "```\n",
    "\n",
    "- Define the structure of the autoencoder model:\n",
    "  - `Input` layer with 10 neurons, representing the input features.\n",
    "  - `Dense` layer with 8 neurons and ReLU activation for encoding.\n",
    "  - Another `Dense` layer with 10 neurons and Sigmoid activation for decoding.\n",
    "  - Create an instance of the `Model` class using the defined input and output layers.\n",
    "\n",
    "```python\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "```\n",
    "\n",
    "- Compile the autoencoder model:\n",
    "  - Use the Adam optimizer.\n",
    "  - Use mean squared error (MSE) as the loss function.\n",
    "\n",
    "```python\n",
    "# Fit Autoencoder model\n",
    "autoencoder.fit(X_scaled, X_scaled, epochs=50, batch_size=32, shuffle=True, validation_split=0.2)\n",
    "```\n",
    "\n",
    "- Train the autoencoder model:\n",
    "  - Fit the model to the standardized data (`X_scaled`).\n",
    "  - Use 50 epochs, a batch size of 32, and shuffle the training data.\n",
    "  - Use 20% of the data for validation.\n",
    "\n",
    "```python\n",
    "# Predict reconstruction errors\n",
    "X_pred = autoencoder.predict(X_scaled)\n",
    "mse = np.mean(np.square(X_scaled - X_pred), axis=1)\n",
    "```\n",
    "\n",
    "- Predict reconstruction errors:\n",
    "  - Use the trained autoencoder to predict reconstructed data (`X_pred`).\n",
    "  - Calculate the mean squared error (MSE) between the original and reconstructed data for each sample.\n",
    "\n",
    "```python\n",
    "# Set a threshold for anomaly detection\n",
    "threshold = np.percentile(mse, 95)\n",
    "```\n",
    "\n",
    "- Set a threshold for anomaly detection:\n",
    "  - Determine a threshold based on the 95th percentile of the MSE values.\n",
    "\n",
    "```python\n",
    "# Predict outliers\n",
    "y_pred_autoencoder = (mse > threshold).astype(int)\n",
    "```\n",
    "\n",
    "- Predict outliers:\n",
    "  - Classify instances as outliers based on whether their MSE is above the threshold.\n",
    "  - Convert the boolean predictions to integers.\n",
    "\n",
    "```python\n",
    "# Print predicted outliers\n",
    "print(\"Autoencoder Predicted Outliers:\", np.where(y_pred_autoencoder == 1))\n",
    "```\n",
    "\n",
    "- Print the predicted outliers:\n",
    "  - Display the indices where the predicted outliers are found based on the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
