{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Boosting**\n",
    "\n",
    "Boosting is an ensemble learning technique that combines the predictions of multiple weak learners (often simple models like decision trees) to create a strong learner. The basic idea is to train models sequentially, with each model focusing on the mistakes of its predecessors. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Boosting Algorithms vs Activation Functions**\n",
    "While both boosting algorithms and activation functions contribute to the learning process in machine learning models, they operate at different levels. Let's clarify their roles first:\n",
    "\n",
    "1. **Boosting Algorithms:**\n",
    "   - Boosting algorithms, as discussed earlier, are ensemble learning techniques that combine the predictions of multiple weak learners to create a strong learner. These algorithms optimize the overall model by sequentially training weak learners and adjusting their contributions based on the errors made by the ensemble. Examples include AdaBoost, Gradient Boosting, XGBoost, and others.\n",
    "   - Boosting is a strategy for improving the overall model performance by emphasizing difficult-to-learn examples and building a strong model from weak ones.\n",
    "\n",
    "   `Operate at the model level`, combining the predictions of multiple models to create a strong ensemble. They are used to improve the overall model's performance by focusing on examples that are difficult to learn.\n",
    "\n",
    "2. **ReLU Activation Function:**\n",
    "   - The Rectified Linear Unit (ReLU) is an activation function commonly used in neural networks. It introduces non-linearity into the model by outputting the input for positive values and zero for negative values. The function is defined as f(x) = max(0, x).\n",
    "   - ReLU and its variants (like leaky ReLU, parametric ReLU, etc.) are used to introduce non-linearities into neural networks, enabling them to learn complex patterns and relationships in the data. They help address the vanishing gradient problem and speed up the convergence of neural networks during training.\n",
    "   \n",
    "   `Operate at the neuron level in neural networks`, introducing non-linearity and enabling the network to learn complex representations. They are used to enhance the learning capacity of individual neurons within the network.\n",
    "\n",
    "\n",
    "\n",
    "In summary, boosting algorithms and activation functions serve complementary roles in machine learning. Boosting focuses on ensemble learning and model combination, while activation functions contribute to the non-linearities and expressiveness of individual models, particularly in the context of neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Types and Common Usecases**\n",
    "\n",
    "Boosting algorithms come in various types, each with its characteristics and use cases. Here are some of the most essential types of boosting algorithms along with their common use cases:\n",
    "\n",
    "1. **AdaBoost (Adaptive Boosting):**\n",
    "   - **Use Case:** Binary classification problems.\n",
    "   - **Key Characteristics:** Assigns weights to misclassified data points and focuses on correcting errors.\n",
    "\n",
    "2. **Gradient Boosting:**\n",
    "   - **Use Cases:**\n",
    "      - Regression problems.\n",
    "      - Classification problems.\n",
    "      - Ranking tasks.\n",
    "   - **Key Characteristics:** Builds trees sequentially, with each tree correcting the errors of the previous ones. Uses gradient descent optimization.\n",
    "\n",
    "3. **XGBoost (Extreme Gradient Boosting):**\n",
    "   - **Use Cases:**\n",
    "      - Large datasets (Commonly seen in competitions such as Kaggle Competitions).\n",
    "      - Regression and classification tasks.\n",
    "   - **Key Characteristics:** Regularized gradient boosting. Parallel and distributed computing for efficiency.\n",
    "\n",
    "4. **LightGBM:**\n",
    "   - **Use Cases:**\n",
    "      - Large datasets.\n",
    "      - Classification and regression tasks.\n",
    "   - **Key Characteristics:** Gradient boosting framework that uses tree-based learning. Efficient with large datasets and supports parallel and distributed training.\n",
    "\n",
    "5. **CatBoost:**\n",
    "   - **Use Cases:**\n",
    "      - Categorical feature-heavy datasets.\n",
    "      - Classification and regression tasks.\n",
    "   - **Key Characteristics:** Handles categorical features efficiently. Robust to overfitting.\n",
    "\n",
    "6. **Stochastic Gradient Boosting:**\n",
    "   - **Use Cases:**\n",
    "      - Regression and classification tasks.\n",
    "      - Large datasets.\n",
    "   - **Key Characteristics:** Introduces randomness by training on random subsets of data. Improves generalization.\n",
    "\n",
    "7. **LogitBoost:**\n",
    "   - **Use Case:** Binary classification problems.\n",
    "   - **Key Characteristics:** Minimizes logistic loss. Similar to AdaBoost but with a focus on logistic regression.\n",
    "\n",
    "8. **LPBoost (Linear Programming Boosting):**\n",
    "   - **Use Cases:**\n",
    "      - Regression and classification tasks.\n",
    "      - Sparse datasets.\n",
    "   - **Key Characteristics:** Formulates boosting as a linear programming problem. Useful for linear models.\n",
    "\n",
    "9. **BrownBoost:**\n",
    "   - **Use Case:** Classification problems.\n",
    "   - **Key Characteristics:** Minimizes the exponential loss. Designed to be robust to outliers.\n",
    "\n",
    "Choosing the right boosting algorithm depends on the specific characteristics of our data and the task at hand. XGBoost, LightGBM, and CatBoost are often popular choices due to their efficiency and effectiveness in various scenarios. If interpretability is crucial, simpler algorithms like AdaBoost may be preferred. It's essential to experiment with different algorithms and tune their hyperparameters based on our specific use case to achieve optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. AdaBoost (Adaptive Boosting):**\n",
    "\n",
    "**Basic Concept:**\n",
    "- AdaBoost assigns weights to data points and adjusts them during training. It gives higher weight to misclassified points, forcing the algorithm to focus on difficult-to-classify instances.\n",
    "- Models are combined with a weighted sum, where more accurate models contribute more to the final prediction.\n",
    "\n",
    "**Example:**\n",
    "Let's consider a binary classification problem where we want to classify points as either +1 or -1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.87\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a weak learner (Decision Tree)\n",
    "base_classifier = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "# Create an AdaBoost classifier\n",
    "adaboost_classifier = AdaBoostClassifier(base_classifier, n_estimators=50, random_state=42)\n",
    "\n",
    "# Train the AdaBoost classifier\n",
    "adaboost_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = adaboost_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Gradient Boosting:**\n",
    "\n",
    "**Basic Concept:**\n",
    "- Gradient Boosting builds models sequentially, where each model corrects errors made by the previous one.\n",
    "- It minimizes a loss function by adding weak learners using gradient descent.\n",
    "\n",
    "**Example:**\n",
    "Consider a regression problem where we want to predict house prices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Gradient Boosting classifier\n",
    "gradient_boosting_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# Train the Gradient Boosting classifier\n",
    "gradient_boosting_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = gradient_boosting_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. XGBoost (Extreme Gradient Boosting):**\n",
    "\n",
    "XGBoost is an optimized and efficient implementation of gradient boosting. It includes regularization terms to control overfitting and parallelization to speed up training.\n",
    "\n",
    "**Example:**\n",
    "Let's use XGBoost for a binary classification problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.90\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an XGBoost classifier\n",
    "xgboost_classifier = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# Train the XGBoost classifier\n",
    "xgboost_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = xgboost_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. LightGBM:**\n",
    "\n",
    "LightGBM is another efficient gradient boosting framework, designed for distributed and efficient training.\n",
    "\n",
    "**Example:**\n",
    "Let's use LightGBM for a regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 1837\n",
      "[LightGBM] [Info] Number of data points in the train set: 12384, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 2.063397\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Mean Squared Error: 0.29\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Load the California housing dataset\n",
    "california_housing = fetch_california_housing()\n",
    "X, y = california_housing.data, california_housing.target\n",
    "\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Create a LightGBM regressor with force_col_wise=True\n",
    "# use verbose=0 to supress the Info Data before MSE\n",
    "lgb_regressor = lgb.LGBMRegressor(n_estimators=100, learning_rate=0.1, \n",
    "                                  max_depth=3, random_state=42, force_col_wise=True)\n",
    "\n",
    "# Train the LightGBM regressor\n",
    "lgb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = lgb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate mean squared error\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This warning from LightGBM indicates that during the training of a decision tree in the boosting process, the algorithm has reached a point where it cannot find any further splits that result in a positive gain. Gain is a measure of how much the split improves the model, and a positive gain means the split is beneficial.\n",
    "\n",
    "In this particular case, the algorithm has likely reached a point where further splits in the tree do not contribute positively to the model's performance. This can happen for various reasons:\n",
    "\n",
    "1. **Overfitting:** The tree has become too deep, capturing noise in the training data and making the model overly complex. In such cases, continuing to grow the tree may not lead to better generalization on unseen data.\n",
    "\n",
    "2. **Insufficient Data:** If the dataset is very small or lacks diversity, the algorithm may struggle to find meaningful splits with positive gain.\n",
    "\n",
    "3. **Hyperparameter Settings:** The choice of hyperparameters, such as the learning rate, maximum depth of the tree, or minimum samples per leaf, can influence the tree-building process.\n",
    "\n",
    "To address this warning, you might consider:\n",
    "\n",
    "- Adjusting hyperparameters, such as reducing the tree depth or increasing regularization.\n",
    "- Increasing the amount of training data if possible.\n",
    "- Tuning other relevant hyperparameters to find a better balance between model complexity and performance on the validation set.\n",
    "\n",
    "It's important to note that while this warning provides information about the training process, it doesn't necessarily mean that the final model will perform poorly. It's a signal to investigate and potentially fine-tune the hyperparameters to achieve a better trade-off between bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. CatBoost:**\n",
    "\n",
    "CatBoost is another powerful gradient boosting library that is particularly effective with categorical features without the need for extensive preprocessing.\n",
    "\n",
    "**Example:**\n",
    "Let's use CatBoost for a binary classification problem.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6235609\ttotal: 166ms\tremaining: 16.4s\n",
      "1:\tlearn: 0.5672800\ttotal: 185ms\tremaining: 9.08s\n",
      "2:\tlearn: 0.5118201\ttotal: 200ms\tremaining: 6.45s\n",
      "3:\tlearn: 0.4652163\ttotal: 206ms\tremaining: 4.95s\n",
      "4:\tlearn: 0.4390851\ttotal: 212ms\tremaining: 4.02s\n",
      "5:\tlearn: 0.4174281\ttotal: 220ms\tremaining: 3.44s\n",
      "6:\tlearn: 0.4007180\ttotal: 224ms\tremaining: 2.98s\n",
      "7:\tlearn: 0.3851958\ttotal: 228ms\tremaining: 2.62s\n",
      "8:\tlearn: 0.3655613\ttotal: 233ms\tremaining: 2.36s\n",
      "9:\tlearn: 0.3548109\ttotal: 237ms\tremaining: 2.13s\n",
      "10:\tlearn: 0.3460026\ttotal: 240ms\tremaining: 1.94s\n",
      "11:\tlearn: 0.3374761\ttotal: 244ms\tremaining: 1.79s\n",
      "12:\tlearn: 0.3306036\ttotal: 247ms\tremaining: 1.65s\n",
      "13:\tlearn: 0.3244453\ttotal: 251ms\tremaining: 1.54s\n",
      "14:\tlearn: 0.3187172\ttotal: 254ms\tremaining: 1.44s\n",
      "15:\tlearn: 0.3150886\ttotal: 257ms\tremaining: 1.35s\n",
      "16:\tlearn: 0.3100579\ttotal: 260ms\tremaining: 1.27s\n",
      "17:\tlearn: 0.3054744\ttotal: 263ms\tremaining: 1.2s\n",
      "18:\tlearn: 0.3012390\ttotal: 268ms\tremaining: 1.14s\n",
      "19:\tlearn: 0.2991627\ttotal: 271ms\tremaining: 1.08s\n",
      "20:\tlearn: 0.2965249\ttotal: 274ms\tremaining: 1.03s\n",
      "21:\tlearn: 0.2933538\ttotal: 277ms\tremaining: 980ms\n",
      "22:\tlearn: 0.2912062\ttotal: 279ms\tremaining: 935ms\n",
      "23:\tlearn: 0.2881766\ttotal: 282ms\tremaining: 894ms\n",
      "24:\tlearn: 0.2762215\ttotal: 287ms\tremaining: 860ms\n",
      "25:\tlearn: 0.2668343\ttotal: 290ms\tremaining: 824ms\n",
      "26:\tlearn: 0.2653530\ttotal: 292ms\tremaining: 790ms\n",
      "27:\tlearn: 0.2639323\ttotal: 295ms\tremaining: 759ms\n",
      "28:\tlearn: 0.2616055\ttotal: 298ms\tremaining: 730ms\n",
      "29:\tlearn: 0.2597642\ttotal: 301ms\tremaining: 702ms\n",
      "30:\tlearn: 0.2583285\ttotal: 304ms\tremaining: 678ms\n",
      "31:\tlearn: 0.2570412\ttotal: 308ms\tremaining: 654ms\n",
      "32:\tlearn: 0.2539980\ttotal: 311ms\tremaining: 631ms\n",
      "33:\tlearn: 0.2463639\ttotal: 313ms\tremaining: 608ms\n",
      "34:\tlearn: 0.2457015\ttotal: 316ms\tremaining: 587ms\n",
      "35:\tlearn: 0.2443584\ttotal: 319ms\tremaining: 567ms\n",
      "36:\tlearn: 0.2422274\ttotal: 322ms\tremaining: 548ms\n",
      "37:\tlearn: 0.2402067\ttotal: 327ms\tremaining: 533ms\n",
      "38:\tlearn: 0.2390181\ttotal: 330ms\tremaining: 516ms\n",
      "39:\tlearn: 0.2370856\ttotal: 334ms\tremaining: 501ms\n",
      "40:\tlearn: 0.2353655\ttotal: 337ms\tremaining: 484ms\n",
      "41:\tlearn: 0.2342087\ttotal: 339ms\tremaining: 469ms\n",
      "42:\tlearn: 0.2327417\ttotal: 343ms\tremaining: 454ms\n",
      "43:\tlearn: 0.2315570\ttotal: 346ms\tremaining: 441ms\n",
      "44:\tlearn: 0.2307241\ttotal: 349ms\tremaining: 427ms\n",
      "45:\tlearn: 0.2294838\ttotal: 352ms\tremaining: 413ms\n",
      "46:\tlearn: 0.2287686\ttotal: 355ms\tremaining: 400ms\n",
      "47:\tlearn: 0.2278675\ttotal: 358ms\tremaining: 388ms\n",
      "48:\tlearn: 0.2256936\ttotal: 364ms\tremaining: 378ms\n",
      "49:\tlearn: 0.2239610\ttotal: 367ms\tremaining: 367ms\n",
      "50:\tlearn: 0.2220806\ttotal: 369ms\tremaining: 355ms\n",
      "51:\tlearn: 0.2196913\ttotal: 373ms\tremaining: 344ms\n",
      "52:\tlearn: 0.2184613\ttotal: 377ms\tremaining: 334ms\n",
      "53:\tlearn: 0.2176716\ttotal: 380ms\tremaining: 324ms\n",
      "54:\tlearn: 0.2172609\ttotal: 384ms\tremaining: 314ms\n",
      "55:\tlearn: 0.2168989\ttotal: 387ms\tremaining: 304ms\n",
      "56:\tlearn: 0.2158797\ttotal: 390ms\tremaining: 295ms\n",
      "57:\tlearn: 0.2144754\ttotal: 394ms\tremaining: 285ms\n",
      "58:\tlearn: 0.2132990\ttotal: 397ms\tremaining: 276ms\n",
      "59:\tlearn: 0.2125810\ttotal: 400ms\tremaining: 267ms\n",
      "60:\tlearn: 0.2112163\ttotal: 404ms\tremaining: 258ms\n",
      "61:\tlearn: 0.2102953\ttotal: 407ms\tremaining: 250ms\n",
      "62:\tlearn: 0.2094250\ttotal: 410ms\tremaining: 241ms\n",
      "63:\tlearn: 0.2085155\ttotal: 413ms\tremaining: 233ms\n",
      "64:\tlearn: 0.2083415\ttotal: 417ms\tremaining: 224ms\n",
      "65:\tlearn: 0.2070122\ttotal: 422ms\tremaining: 218ms\n",
      "66:\tlearn: 0.2054931\ttotal: 429ms\tremaining: 211ms\n",
      "67:\tlearn: 0.2024839\ttotal: 450ms\tremaining: 212ms\n",
      "68:\tlearn: 0.2004561\ttotal: 467ms\tremaining: 210ms\n",
      "69:\tlearn: 0.1992872\ttotal: 474ms\tremaining: 203ms\n",
      "70:\tlearn: 0.1976172\ttotal: 477ms\tremaining: 195ms\n",
      "71:\tlearn: 0.1961914\ttotal: 482ms\tremaining: 188ms\n",
      "72:\tlearn: 0.1955678\ttotal: 487ms\tremaining: 180ms\n",
      "73:\tlearn: 0.1914231\ttotal: 491ms\tremaining: 172ms\n",
      "74:\tlearn: 0.1897005\ttotal: 495ms\tremaining: 165ms\n",
      "75:\tlearn: 0.1880822\ttotal: 499ms\tremaining: 157ms\n",
      "76:\tlearn: 0.1871844\ttotal: 505ms\tremaining: 151ms\n",
      "77:\tlearn: 0.1863009\ttotal: 508ms\tremaining: 143ms\n",
      "78:\tlearn: 0.1817900\ttotal: 511ms\tremaining: 136ms\n",
      "79:\tlearn: 0.1810907\ttotal: 515ms\tremaining: 129ms\n",
      "80:\tlearn: 0.1806316\ttotal: 518ms\tremaining: 122ms\n",
      "81:\tlearn: 0.1770297\ttotal: 525ms\tremaining: 115ms\n",
      "82:\tlearn: 0.1763136\ttotal: 528ms\tremaining: 108ms\n",
      "83:\tlearn: 0.1759679\ttotal: 531ms\tremaining: 101ms\n",
      "84:\tlearn: 0.1757916\ttotal: 534ms\tremaining: 94.3ms\n",
      "85:\tlearn: 0.1747269\ttotal: 537ms\tremaining: 87.4ms\n",
      "86:\tlearn: 0.1744857\ttotal: 542ms\tremaining: 81ms\n",
      "87:\tlearn: 0.1730814\ttotal: 545ms\tremaining: 74.4ms\n",
      "88:\tlearn: 0.1722329\ttotal: 549ms\tremaining: 67.9ms\n",
      "89:\tlearn: 0.1720920\ttotal: 553ms\tremaining: 61.5ms\n",
      "90:\tlearn: 0.1711165\ttotal: 556ms\tremaining: 55ms\n",
      "91:\tlearn: 0.1707653\ttotal: 561ms\tremaining: 48.8ms\n",
      "92:\tlearn: 0.1698130\ttotal: 565ms\tremaining: 42.5ms\n",
      "93:\tlearn: 0.1692891\ttotal: 569ms\tremaining: 36.3ms\n",
      "94:\tlearn: 0.1683252\ttotal: 573ms\tremaining: 30.2ms\n",
      "95:\tlearn: 0.1666754\ttotal: 576ms\tremaining: 24ms\n",
      "96:\tlearn: 0.1656400\ttotal: 584ms\tremaining: 18.1ms\n",
      "97:\tlearn: 0.1646132\ttotal: 587ms\tremaining: 12ms\n",
      "98:\tlearn: 0.1643889\ttotal: 591ms\tremaining: 5.97ms\n",
      "99:\tlearn: 0.1639458\ttotal: 594ms\tremaining: 0us\n",
      "Accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a CatBoost classifier\n",
    "catboost_classifier = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=3, random_state=42)\n",
    "\n",
    "# Train the CatBoost classifier\n",
    "catboost_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = catboost_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6. Stacking:**\n",
    "\n",
    "Stacking is a different approach to ensemble learning where multiple models are trained, and their predictions are used as input features for a final meta-model.\n",
    "\n",
    "**Example:**\n",
    "Let's stack a Random Forest, AdaBoost, and XGBoost for a binary classification problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.90\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define base models\n",
    "base_models = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=50, random_state=42)),\n",
    "    ('adaboost', AdaBoostClassifier(n_estimators=50, random_state=42)),\n",
    "    ('xgboost', xgb.XGBClassifier(n_estimators=50, random_state=42))\n",
    "]\n",
    "\n",
    "# Define the meta-model\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Create the stacking classifier\n",
    "stacking_classifier = StackingClassifier(estimators=base_models, final_estimator=meta_model)\n",
    "\n",
    "# Train the stacking classifier\n",
    "stacking_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = stacking_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, `base_models` are trained individually, and their predictions are used as input features for the `meta_model`. Stacking can be a powerful technique when different models capture different aspects of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Checking Accuracy with One Boosting Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Testing Set: 0.91\n",
      "Best Hyperparameters: {'n_estimators': 100, 'min_samples_split': 6, 'max_depth': 5, 'learning_rate': 0.2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a synthetic classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 4, 6]\n",
    "}\n",
    "\n",
    "# Create a GradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Perform Randomized Search for hyperparameter tuning\n",
    "random_search = RandomizedSearchCV(gbc, param_distributions=param_grid, n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_model = GradientBoostingClassifier(**best_params, random_state=42)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy on the testing set\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy on Testing Set: {accuracy:.2f}\")\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
