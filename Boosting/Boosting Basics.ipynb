{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Boosting**\n",
    "\n",
    "Boosting is an ensemble learning technique that combines the predictions of multiple weak learners (often simple models like decision trees) to create a strong learner. The basic idea is to train models sequentially, with each model focusing on the mistakes of its predecessors. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Boosting Algorithms vs Activation Functions**\n",
    "While both boosting algorithms and activation functions contribute to the learning process in machine learning models, they operate at different levels. Let's clarify their roles first:\n",
    "\n",
    "1. **Boosting Algorithms:**\n",
    "   - Boosting algorithms, as discussed earlier, are ensemble learning techniques that combine the predictions of multiple weak learners to create a strong learner. These algorithms optimize the overall model by sequentially training weak learners and adjusting their contributions based on the errors made by the ensemble. Examples include AdaBoost, Gradient Boosting, XGBoost, and others.\n",
    "   - Boosting is a strategy for improving the overall model performance by emphasizing difficult-to-learn examples and building a strong model from weak ones.\n",
    "\n",
    "   `Operate at the model level`, combining the predictions of multiple models to create a strong ensemble. They are used to improve the overall model's performance by focusing on examples that are difficult to learn.\n",
    "\n",
    "2. **ReLU Activation Function:**\n",
    "   - The Rectified Linear Unit (ReLU) is an activation function commonly used in neural networks. It introduces non-linearity into the model by outputting the input for positive values and zero for negative values. The function is defined as f(x) = max(0, x).\n",
    "   - ReLU and its variants (like leaky ReLU, parametric ReLU, etc.) are used to introduce non-linearities into neural networks, enabling them to learn complex patterns and relationships in the data. They help address the vanishing gradient problem and speed up the convergence of neural networks during training.\n",
    "   \n",
    "   `Operate at the neuron level in neural networks`, introducing non-linearity and enabling the network to learn complex representations. They are used to enhance the learning capacity of individual neurons within the network.\n",
    "\n",
    "\n",
    "\n",
    "In summary, boosting algorithms and activation functions serve complementary roles in machine learning. Boosting focuses on ensemble learning and model combination, while activation functions contribute to the non-linearities and expressiveness of individual models, particularly in the context of neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Types and Common Usecases**\n",
    "\n",
    "Boosting algorithms come in various types, each with its characteristics and use cases. Here are some of the most essential types of boosting algorithms along with their common use cases:\n",
    "\n",
    "1. **AdaBoost (Adaptive Boosting):**\n",
    "   - **Use Case:** Binary classification problems.\n",
    "   - **Key Characteristics:** Assigns weights to misclassified data points and focuses on correcting errors.\n",
    "\n",
    "2. **Gradient Boosting:**\n",
    "   - **Use Cases:**\n",
    "      - Regression problems.\n",
    "      - Classification problems.\n",
    "      - Ranking tasks.\n",
    "   - **Key Characteristics:** Builds trees sequentially, with each tree correcting the errors of the previous ones. Uses gradient descent optimization.\n",
    "\n",
    "3. **XGBoost (Extreme Gradient Boosting):**\n",
    "   - **Use Cases:**\n",
    "      - Large datasets (Commonly seen in competitions such as Kaggle Competitions).\n",
    "      - Regression and classification tasks.\n",
    "   - **Key Characteristics:** Regularized gradient boosting. Parallel and distributed computing for efficiency.\n",
    "\n",
    "4. **LightGBM:**\n",
    "   - **Use Cases:**\n",
    "      - Large datasets.\n",
    "      - Classification and regression tasks.\n",
    "   - **Key Characteristics:** Gradient boosting framework that uses tree-based learning. Efficient with large datasets and supports parallel and distributed training.\n",
    "\n",
    "5. **CatBoost:**\n",
    "   - **Use Cases:**\n",
    "      - Categorical feature-heavy datasets.\n",
    "      - Classification and regression tasks.\n",
    "   - **Key Characteristics:** Handles categorical features efficiently. Robust to overfitting.\n",
    "\n",
    "6. **Stochastic Gradient Boosting:**\n",
    "   - **Use Cases:**\n",
    "      - Regression and classification tasks.\n",
    "      - Large datasets.\n",
    "   - **Key Characteristics:** Introduces randomness by training on random subsets of data. Improves generalization.\n",
    "\n",
    "7. **LogitBoost:**\n",
    "   - **Use Case:** Binary classification problems.\n",
    "   - **Key Characteristics:** Minimizes logistic loss. Similar to AdaBoost but with a focus on logistic regression.\n",
    "\n",
    "8. **LPBoost (Linear Programming Boosting):**\n",
    "   - **Use Cases:**\n",
    "      - Regression and classification tasks.\n",
    "      - Sparse datasets.\n",
    "   - **Key Characteristics:** Formulates boosting as a linear programming problem. Useful for linear models.\n",
    "\n",
    "9. **BrownBoost:**\n",
    "   - **Use Case:** Classification problems.\n",
    "   - **Key Characteristics:** Minimizes the exponential loss. Designed to be robust to outliers.\n",
    "\n",
    "Choosing the right boosting algorithm depends on the specific characteristics of our data and the task at hand. XGBoost, LightGBM, and CatBoost are often popular choices due to their efficiency and effectiveness in various scenarios. If interpretability is crucial, simpler algorithms like AdaBoost may be preferred. It's essential to experiment with different algorithms and tune their hyperparameters based on our specific use case to achieve optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. AdaBoost (Adaptive Boosting):**\n",
    "\n",
    "**Basic Concept:**\n",
    "- AdaBoost assigns weights to data points and adjusts them during training. It gives higher weight to misclassified points, forcing the algorithm to focus on difficult-to-classify instances.\n",
    "- Models are combined with a weighted sum, where more accurate models contribute more to the final prediction.\n",
    "\n",
    "**Example:**\n",
    "Let's consider a binary classification problem where we want to classify points as either +1 or -1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.87\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a weak learner (Decision Tree)\n",
    "base_classifier = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "# Create an AdaBoost classifier\n",
    "adaboost_classifier = AdaBoostClassifier(base_classifier, n_estimators=50, random_state=42)\n",
    "\n",
    "# Train the AdaBoost classifier\n",
    "adaboost_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = adaboost_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Gradient Boosting:**\n",
    "\n",
    "**Basic Concept:**\n",
    "- Gradient Boosting builds models sequentially, where each model corrects errors made by the previous one.\n",
    "- It minimizes a loss function by adding weak learners using gradient descent.\n",
    "\n",
    "**Example:**\n",
    "Consider a regression problem where we want to predict house prices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Gradient Boosting classifier\n",
    "gradient_boosting_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# Train the Gradient Boosting classifier\n",
    "gradient_boosting_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = gradient_boosting_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
