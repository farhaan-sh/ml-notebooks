{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Deep learning encompasses various algorithms**\n",
    "\n",
    "1. **Feedforward Neural Networks (FNN) / Multi-layer Perceptrons (MLP):**\n",
    "   - **Structure:** Composed of an input layer, one or more hidden layers, and an output layer. Neurons in each layer are fully connected to neurons in the subsequent layer.\n",
    "   - **Example:** Used for tasks like image classification. The MNIST dataset is a classic example where FNNs are applied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_clusters_per_class=2, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the MLPClassifier\n",
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(50, 20), max_iter=1000, random_state=42)\n",
    "mlp_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = mlp_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of the code:\n",
    "\n",
    "1. We use `make_classification` to generate a synthetic dataset with 1000 samples, 20 features, and 2 informative features.\n",
    "\n",
    "2. The dataset is split into training and testing sets using `train_test_split`.\n",
    "\n",
    "3. We initialize an `MLPClassifier` with two hidden layers containing 50 and 20 neurons, respectively. The `max_iter` parameter controls the maximum number of iterations (epochs) for training.\n",
    "\n",
    "4. The model is trained on the training set using the `fit` method.\n",
    "\n",
    "5. Predictions are made on the test set using the `predict` method.\n",
    "\n",
    "6. The accuracy of the model is evaluated using the `accuracy_score` from `sklearn.metrics`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Train the model and collect loss values\n",
    "history = mlp_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.plot(history.loss_curve_)\n",
    "plt.title('Training Loss Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract weights from the first hidden layer\n",
    "weights = mlp_classifier.coefs_[0]\n",
    "\n",
    "# Plot the weights\n",
    "plt.imshow(weights, cmap='viridis', aspect='auto')\n",
    "plt.title('Weights of the First Hidden Layer')\n",
    "plt.xlabel('Neurons in the Previous Layer')\n",
    "plt.ylabel('Neurons in the Current Layer')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. **Convolutional Neural Networks (CNN):**\n",
    "   - **Structure:** Employs convolutional layers to automatically and adaptively learn spatial hierarchies of features. Common components include convolutional layers, pooling layers, and fully connected layers.\n",
    "   - **Example:** Widely used in image recognition tasks. Examples include AlexNet, VGGNet, and ResNet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_clusters_per_class=2, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reshape the data for CNN input (assuming a 2D input)\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1, 1)\n",
    "\n",
    "# Build the CNN model\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional layer with 32 filters, kernel size 3x1, and activation function 'relu'\n",
    "model.add(Conv2D(32, (3, 1), activation='relu', input_shape=(X_train.shape[1], 1, 1)))\n",
    "\n",
    "\n",
    "# Max pooling layer\n",
    "model.add(MaxPooling2D(pool_size=(2, 1)))\n",
    "\n",
    "# Flatten layer to convert 2D data to a vector\n",
    "model.add(Flatten())\n",
    "\n",
    "# Dense (fully connected) layer with 128 neurons and activation function 'relu'\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# Output layer with 1 neuron (binary classification) and activation function 'sigmoid'\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'\\nTest Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. **Recurrent Neural Networks (RNN):**\n",
    "   - **Structure:** Designed to handle sequential data by introducing connections that form directed cycles. Can have a simple RNN structure or more advanced variants like Long Short-Term Memory (LSTM) or Gated Recurrent Unit (GRU).\n",
    "   - **Example:** Applied in natural language processing tasks, speech recognition, and time series prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_multilabel_classification(n_samples=1000, n_features=10, n_classes=5, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape the input data for RNN\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Build the RNN model with 5 layers\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=50, activation='relu', input_shape=(1, X_train.shape[2]), return_sequences=True))\n",
    "model.add(SimpleRNN(units=50, activation='relu', return_sequences=True))\n",
    "model.add(SimpleRNN(units=50, activation='relu', return_sequences=True))\n",
    "model.add(SimpleRNN(units=50, activation='relu', return_sequences=True))\n",
    "model.add(SimpleRNN(units=50, activation='relu'))\n",
    "model.add(Dense(units=5, activation='sigmoid'))  # Assuming 5 output classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Long Short-Term Memory (LSTM):**\n",
    "   - **Structure:** An RNN variant with memory cells and gates to control the flow of information. Effective for capturing long-range dependencies in sequential data.\n",
    "   - **Example:** Used in language modeling, machine translation, and speech recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=8, n_redundant=2, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape the input data for LSTM (assuming a time series structure)\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Build the LSTM model with 5 layers\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(LSTM(units=50, return_sequences=True))\n",
    "model.add(LSTM(units=50, return_sequences=True))\n",
    "model.add(LSTM(units=50, return_sequences=True))\n",
    "model.add(LSTM(units=50))  # Last layer without return_sequences=True\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Gated Recurrent Unit (GRU):**\n",
    "   - **Structure:** Similar to LSTM but with a simplified architecture. It also includes gates to regulate the flow of information.\n",
    "   - **Example:** Applied in tasks where memory of long-range dependencies is crucial but with fewer parameters compared to LSTM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=1, noise=0.1, random_state=42)\n",
    "\n",
    "# Reshape the data for input to the GRU network\n",
    "X = X.reshape((X.shape[0], 1, X.shape[1]))\n",
    "\n",
    "# Normalize the input data\n",
    "scaler_x = MinMaxScaler(feature_range=(-1, 1))\n",
    "X = scaler_x.fit_transform(X.reshape(-1, 1)).reshape(X.shape)\n",
    "\n",
    "# Normalize the target variable\n",
    "scaler_y = MinMaxScaler(feature_range=(-1, 1))\n",
    "y = scaler_y.fit_transform(y.reshape(-1, 1)).reshape(y.shape)\n",
    "\n",
    "# Define and build the GRU model with 5 layers\n",
    "model = Sequential()\n",
    "model.add(GRU(units=50, activation='tanh', return_sequences=True, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(GRU(units=50, activation='tanh', return_sequences=True))\n",
    "model.add(GRU(units=50, activation='tanh', return_sequences=True))\n",
    "model.add(GRU(units=50, activation='tanh', return_sequences=True))\n",
    "model.add(GRU(units=50, activation='tanh', return_sequences=False))\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Generate new data for prediction (similar to training data for simplicity)\n",
    "X_new = scaler_x.transform(np.linspace(-1, 1, 100).reshape(-1, 1)).reshape((100, 1, 1))\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_new)\n",
    "\n",
    "# Invert the normalization for visualization\n",
    "predictions = scaler_y.inverse_transform(predictions.reshape(-1, 1)).reshape(predictions.shape)\n",
    "\n",
    "# Plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.linspace(-1, 1, 100), predictions, label='Predictions')\n",
    "plt.scatter(X[:, 0, 0], y, color='red', label='Original Data')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Autoencoders:**\n",
    "   - **Structure:** Composed of an encoder that maps input data to a lower-dimensional representation (encoding), and a decoder that reconstructs the input from the encoding. Variational Autoencoders (VAE) introduce probabilistic elements.\n",
    "   - **Example:** Used for dimensionality reduction, anomaly detection, and generative tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. **Generative Adversarial Networks (GAN):**\n",
    "   - **Structure:** Consists of a generator that creates synthetic data and a discriminator that distinguishes between real and synthetic data. Both networks are trained simultaneously in a competitive manner.\n",
    "   - **Example:** Applied in image generation, style transfer, and data augmentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. **Transformer:**\n",
    "   - **Structure:** Self-attention mechanism is used to capture global dependencies in input data. Composed of an encoder and a decoder, often employed in natural language processing.\n",
    "   - **Example:** Transformer architecture is the foundation of models like BERT (Bidirectional Encoder Representations from Transformers) for language understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are just a few examples, and there are many other architectures and variations within deep learning. The choice of algorithm depends on the specific task, the nature of the data, and computational resources available. Additionally, ongoing research in the field continues to introduce new architectures and improvements to existing ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
