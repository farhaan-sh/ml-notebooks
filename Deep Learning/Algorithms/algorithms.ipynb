{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Deep learning encompasses various algorithms**\n",
    "\n",
    "1. **Feedforward Neural Networks (FNN) / Multi-layer Perceptrons (MLP):**\n",
    "   - **Structure:** Composed of an input layer, one or more hidden layers, and an output layer. Neurons in each layer are fully connected to neurons in the subsequent layer.\n",
    "   - **Example:** Used for tasks like image classification. The MNIST dataset is a classic example where FNNs are applied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. **Convolutional Neural Networks (CNN):**\n",
    "   - **Structure:** Employs convolutional layers to automatically and adaptively learn spatial hierarchies of features. Common components include convolutional layers, pooling layers, and fully connected layers.\n",
    "   - **Example:** Widely used in image recognition tasks. Examples include AlexNet, VGGNet, and ResNet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. **Recurrent Neural Networks (RNN):**\n",
    "   - **Structure:** Designed to handle sequential data by introducing connections that form directed cycles. Can have a simple RNN structure or more advanced variants like Long Short-Term Memory (LSTM) or Gated Recurrent Unit (GRU).\n",
    "   - **Example:** Applied in natural language processing tasks, speech recognition, and time series prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Long Short-Term Memory (LSTM):**\n",
    "   - **Structure:** An RNN variant with memory cells and gates to control the flow of information. Effective for capturing long-range dependencies in sequential data.\n",
    "   - **Example:** Used in language modeling, machine translation, and speech recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Gated Recurrent Unit (GRU):**\n",
    "   - **Structure:** Similar to LSTM but with a simplified architecture. It also includes gates to regulate the flow of information.\n",
    "   - **Example:** Applied in tasks where memory of long-range dependencies is crucial but with fewer parameters compared to LSTM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Autoencoders:**\n",
    "   - **Structure:** Composed of an encoder that maps input data to a lower-dimensional representation (encoding), and a decoder that reconstructs the input from the encoding. Variational Autoencoders (VAE) introduce probabilistic elements.\n",
    "   - **Example:** Used for dimensionality reduction, anomaly detection, and generative tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. **Generative Adversarial Networks (GAN):**\n",
    "   - **Structure:** Consists of a generator that creates synthetic data and a discriminator that distinguishes between real and synthetic data. Both networks are trained simultaneously in a competitive manner.\n",
    "   - **Example:** Applied in image generation, style transfer, and data augmentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. **Transformer:**\n",
    "   - **Structure:** Self-attention mechanism is used to capture global dependencies in input data. Composed of an encoder and a decoder, often employed in natural language processing.\n",
    "   - **Example:** Transformer architecture is the foundation of models like BERT (Bidirectional Encoder Representations from Transformers) for language understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are just a few examples, and there are many other architectures and variations within deep learning. The choice of algorithm depends on the specific task, the nature of the data, and computational resources available. Additionally, ongoing research in the field continues to introduce new architectures and improvements to existing ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
